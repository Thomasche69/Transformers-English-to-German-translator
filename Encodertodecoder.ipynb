{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English/German tokenizers\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [token.text for token in spacy_de.tokenizer(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('wmt14_translate_de-en_train.csv',on_bad_lines='skip', engine='python' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_data = pd.read_csv('wmt14_translate_de-en_validation.csv')\n",
    "test_data = pd.read_csv('wmt14_translate_de-en_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =train_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.sample(n = 1000000, random_state = 42).reset_index(drop= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Define special tokens\n",
    "SOS_TOKEN = \"<sos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, tokens, min_freq=2):\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "        self._build_vocab(tokens, min_freq)\n",
    "\n",
    "    def _build_vocab(self, tokens, min_freq):\n",
    "        counter = Counter(tokens)\n",
    "        vocab = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN] + [\n",
    "            token for token, count in counter.items() if count >= min_freq\n",
    "        ]\n",
    "        for idx, token in enumerate(vocab):\n",
    "            self.stoi[token] = idx\n",
    "            self.itos[idx] = token\n",
    "\n",
    "# Build vocab for English and German\n",
    "en_tokens = [token for sent in train_data['en'] for token in tokenize_en(sent)]\n",
    "de_tokens = [token for sent in train_data['de'] for token in tokenize_de(sent)]\n",
    "\n",
    "en_vocab = Vocab(en_tokens)\n",
    "de_vocab = Vocab(de_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, df, en_vocab, de_vocab):\n",
    "        self.df = df\n",
    "        self.en_vocab = en_vocab\n",
    "        self.de_vocab = de_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        en_sent = self.df.iloc[idx]['en']\n",
    "        de_sent = self.df.iloc[idx]['de']\n",
    "\n",
    "        en_tokens = [SOS_TOKEN] + tokenize_en(en_sent) + [EOS_TOKEN]\n",
    "        de_tokens = [SOS_TOKEN] + tokenize_de(de_sent) + [EOS_TOKEN]\n",
    "\n",
    "        en_indices = [en_vocab.stoi.get(token, en_vocab.stoi[UNK_TOKEN]) \n",
    "                     for token in en_tokens]\n",
    "        de_indices = [de_vocab.stoi.get(token, de_vocab.stoi[UNK_TOKEN]) \n",
    "                     for token in de_tokens]\n",
    "\n",
    "        return torch.tensor(en_indices, dtype=torch.long), \\\n",
    "               torch.tensor(de_indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    en_batch, de_batch = zip(*batch)\n",
    "    en_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        en_batch, padding_value=en_vocab.stoi[PAD_TOKEN]\n",
    "    )\n",
    "    de_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        de_batch, padding_value=de_vocab.stoi[PAD_TOKEN]\n",
    "    )\n",
    "    return en_padded, de_padded\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TranslationDataset(train_data, en_vocab, de_vocab)\n",
    "val_dataset = TranslationDataset(val_data, en_vocab, de_vocab)\n",
    "test_dataset = TranslationDataset(test_data, en_vocab, de_vocab)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, \n",
    "    shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, \n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self,vocab_size, d_model):\n",
    "        super(InputEmbeddings, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.embed(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model,max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length,dtype =torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe',pe.unsqueeze(0))\n",
    "    def forward(self,x):\n",
    "        return x + self.pe[:, :x.size(1)] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.query_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.key_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # x: [batch, seq_len, d_model]\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        return x.transpose(1, 2)  # [batch, num_heads, seq_len, head_dim]\n",
    "    \n",
    "    def compute_attention(self, query, key, value, mask=None):\n",
    "        # scores: [batch, num_heads, query_len, key_len]\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # Adjust mask dimensions based on its number of dimensions\n",
    "            if mask.dim() == 2:  # e.g. [batch, key_len] -> key padding mask\n",
    "                mask = mask.unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, key_len]\n",
    "            elif mask.dim() == 3:  # e.g. [batch, query_len, key_len]\n",
    "                mask = mask.unsqueeze(1)  # [batch, 1, query_len, key_len]\n",
    "            # Fill masked positions with -infinity\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        return torch.matmul(attention, value)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        # x: [batch, num_heads, seq_len, head_dim]\n",
    "        batch_size, num_heads, seq_len, head_dim = x.size()\n",
    "        x = x.transpose(1, 2).contiguous()  # [batch, seq_len, num_heads, head_dim]\n",
    "        return x.view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query, key, value: [batch, seq_len, d_model]\n",
    "        query = self.split_heads(self.query_linear(query))\n",
    "        key   = self.split_heads(self.key_linear(key))\n",
    "        value = self.split_heads(self.value_linear(value))\n",
    "        attention = self.compute_attention(query, key, value, mask)\n",
    "        output = self.combine_heads(attention)\n",
    "        return self.output_linear(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardSubLayer(nn.Module):\n",
    "    def __init__(self,d_model,d_ff):\n",
    "        super(FeedForwardSubLayer, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        return self.linear2(self.relu(self.linear1(x)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x,src_mask):\n",
    "        attn_output = self.multi_head_attention(x,x,x,src_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads,d_ff,dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff_sublayer = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.Dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x,enc_output,tgt_mask,cross_mask): \n",
    "        self_attn_output = self.self_attn(x,x,x,tgt_mask)\n",
    "        x = self.norm1(x + self.Dropout(self_attn_output))\n",
    "        cross_attn_output = self.cross_attn(x,enc_output,enc_output,cross_mask)\n",
    "        x = self.norm2(x + self.Dropout(cross_attn_output))\n",
    "        ff_output = self.ff_sublayer(x)\n",
    "        x = self.norm3(x + self.Dropout(ff_output))\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size,d_model,num_layers,num_heads,d_ff,dropout,max_seq_length):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = InputEmbeddings(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model,max_seq_length)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model,num_heads,d_ff,dropout) for _ in range(num_layers)])\n",
    "    def forward(self,x,src_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,src_mask)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model,num_layers,num_heads,d_ff,dropout,max_seq_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = InputEmbeddings(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model,max_seq_length)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model,num_heads,d_ff,dropout) for _ in range(num_layers)])\n",
    "        self.linear = nn.Linear(d_model,vocab_size)\n",
    "    def forward(self,x,enc_output,tgt_mask,cross_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,enc_output,tgt_mask,cross_mask)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,vocab_size_en,vocab_size_de,d_model,num_heads,num_layers,d_ff,max_seq_length,dropout):\n",
    "        super(Transformer,self).__init__()\n",
    "        self.encoder = TransformerEncoder(vocab_size_en,d_model,num_layers,num_heads,d_ff,dropout,max_seq_length)\n",
    "        self.decoder = TransformerDecoder(vocab_size_de,d_model,num_layers,num_heads,d_ff,dropout,max_seq_length)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, cross_mask):\n",
    "        encoder_output = self.encoder(src, src_mask) \n",
    "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, cross_mask) \n",
    "        decoder_output = F.log_softmax(decoder_output, dim=-1)\n",
    "        return decoder_output\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Transformer(\n",
    "    len(en_vocab.stoi), len(de_vocab.stoi),\n",
    "    d_model=512, num_heads=8,\n",
    "    num_layers = 6, d_ff=2048, max_seq_length=256,dropout = 0.1\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "val_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, device, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Train a PyTorch model and display train and test loss at each epoch.\n",
    "    Assumes the model's forward signature is:\n",
    "      forward(src, tgt, src_mask, tgt_mask, cross_mask)\n",
    "    \"\"\"\n",
    "    # Move the model to the specified device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=de_vocab.stoi[PAD_TOKEN])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "    # Initialize history lists\n",
    "    loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for src, tgt in train_loader:\n",
    "            # Ensure that src and tgt have the same batch size.\n",
    "            # If not, you may want to skip or fix the batch.\n",
    "            if src.size(0) != tgt.size(0):\n",
    "                continue  # or raise an error\n",
    "\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            # Build masks from actual input tensor sizes:\n",
    "            src_mask = (src != en_vocab.stoi[PAD_TOKEN]).unsqueeze(1)  # shape: [batch, 1, src_seq_len]\n",
    "\n",
    "            # Prepare decoder input and target (teacher forcing)\n",
    "            decoder_input = tgt[:, :-1]  # [batch, tgt_seq_len - 1]\n",
    "            target = tgt[:, 1:]          # [batch, tgt_seq_len - 1]\n",
    "\n",
    "            # Build decoder mask based on the decoder input\n",
    "            tgt_mask = (decoder_input != de_vocab.stoi[PAD_TOKEN]).unsqueeze(1)  # shape: [batch, 1, tgt_seq_len - 1]\n",
    "\n",
    "            # For cross-attention, use the encoder mask (adjust if needed)\n",
    "            cross_mask = src_mask\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(src, decoder_input, src_mask, tgt_mask, cross_mask)\n",
    "            # Expected output shape: [batch, tgt_seq_len - 1, vocab_size]\n",
    "            \n",
    "            # Use .reshape instead of .view() to avoid contiguous issues.\n",
    "            loss = criterion(output.reshape(-1, output.shape[-1]), target.reshape(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in val_loader:\n",
    "                if src.size(0) != tgt.size(0):\n",
    "                    continue\n",
    "\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "                src_mask = (src != en_vocab.stoi[PAD_TOKEN]).unsqueeze(1)\n",
    "\n",
    "                decoder_input = tgt[:, :-1]\n",
    "                target = tgt[:, 1:]\n",
    "                tgt_mask = (decoder_input != de_vocab.stoi[PAD_TOKEN]).unsqueeze(1)\n",
    "                cross_mask = src_mask\n",
    "\n",
    "                output = model(src, decoder_input, src_mask, tgt_mask, cross_mask)\n",
    "                loss = criterion(output.reshape(-1, output.shape[-1]), target.reshape(-1))\n",
    "                val_running_loss += loss.item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        val_loss = val_running_loss / len(val_loader)\n",
    "        loss_history.append(train_loss)\n",
    "        val_loss_history.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] - Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.08 GiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 11.87 GiB is allocated by PyTorch, and 5.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 50\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model, train_loader, val_loader, device, num_epochs)\u001b[0m\n\u001b[0;32m     48\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), target\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     49\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 50\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     52\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\aalen\\OneDrive\\桌面\\EE Transformer\\.venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aalen\\OneDrive\\桌面\\EE Transformer\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aalen\\OneDrive\\桌面\\EE Transformer\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.08 GiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 11.87 GiB is allocated by PyTorch, and 5.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(model, train_loader, val_loader, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "\n",
    "def calculate_bleu(model, val_loader, device, max_len=50):\n",
    "    model.eval()\n",
    "    hypotheses = []\n",
    "    references = []\n",
    "    \n",
    "    sos_token = de_vocab.stoi[SOS_TOKEN]\n",
    "    eos_token = de_vocab.stoi[EOS_TOKEN]\n",
    "    pad_token = de_vocab.stoi[PAD_TOKEN]\n",
    "    \n",
    "    for src, tgt in val_loader:\n",
    "        # Add batch size validation like in training\n",
    "        if src.size(0) != tgt.size(0):\n",
    "            continue  # Skip mismatched batches\n",
    "            \n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)  # Move tgt to device for consistency\n",
    "        batch_size = src.size(0)\n",
    "        \n",
    "        src_mask = (src != en_vocab.stoi[PAD_TOKEN]).unsqueeze(1)\n",
    "        decoder_input = torch.full((batch_size, 1), sos_token, \n",
    "                                 dtype=torch.long, device=device)\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            tgt_mask = (decoder_input != pad_token).unsqueeze(1)\n",
    "            output = model(src, decoder_input, src_mask, tgt_mask, src_mask)\n",
    "            next_token = output.argmax(dim=-1)[:, -1].unsqueeze(1)\n",
    "            decoder_input = torch.cat([decoder_input, next_token], dim=-1)\n",
    "            \n",
    "            if (next_token == eos_token).all():\n",
    "                break\n",
    "\n",
    "        # Process batch elements with explicit size check\n",
    "        for i in range(tgt.size(0)):  # Use tgt's batch size directly\n",
    "            # Handle hypothesis\n",
    "            hyp_sequence = decoder_input[i, 1:].cpu().tolist()\n",
    "            hyp_tokens = [\n",
    "                de_vocab.itos[idx] \n",
    "                for idx in hyp_sequence \n",
    "                if idx not in {eos_token, pad_token}\n",
    "            ]\n",
    "            if eos_token in hyp_sequence:\n",
    "                hyp_tokens = hyp_tokens[:hyp_sequence.index(eos_token)]\n",
    "            hypotheses.append(hyp_tokens)\n",
    "            \n",
    "            # Handle reference with safe slicing\n",
    "            ref_sequence = tgt[i, 1:].cpu().tolist()  # Now guaranteed to exist\n",
    "            ref_tokens = [\n",
    "                de_vocab.itos[idx] \n",
    "                for idx in ref_sequence \n",
    "                if idx not in {eos_token, pad_token}\n",
    "            ]\n",
    "            if eos_token in ref_sequence:\n",
    "                ref_tokens = ref_tokens[:ref_sequence.index(eos_token)]\n",
    "            references.append([ref_tokens])  # Wrap in list for corpus_bleu\n",
    "\n",
    "    bleu_score = corpus_bleu(references, hypotheses,\n",
    "                            weights=(0.25, 0.25, 0.25, 0.25),\n",
    "                            smoothing_function=SmoothingFunction().method1)\n",
    "    return bleu_score\n",
    "\n",
    "# Usage example after training:\n",
    "bleu_score = calculate_bleu(model, test_loader, device)\n",
    "print(f\"BLEU-4 Score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, en_vocab, de_vocab, device, max_length=50):\n",
    "    \"\"\"\n",
    "    Translate a single sentence using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained transformer model\n",
    "        sentence: Input sentence string (source language)\n",
    "        en_vocab: Source vocabulary (English)\n",
    "        de_vocab: Target vocabulary (German)\n",
    "        device: CUDA/CPU device\n",
    "        max_length: Maximum generation length\n",
    "        \n",
    "    Returns:\n",
    "        Translated sentence string (target language)\n",
    "    \"\"\"\n",
    "    # Tokenize and numericalize the source sentence\n",
    "    tokens = [token.lower().strip() for token in sentence.split()]  # Use same preprocessing as training\n",
    "    tokens = [SOS_TOKEN] + tokens + [EOS_TOKEN]\n",
    "    \n",
    "    # Convert to indices using source vocab\n",
    "    src_indices = [en_vocab.stoi[token] if token in en_vocab.stoi \n",
    "                   else en_vocab.stoi[UNK_TOKEN]  # Handle unknown tokens\n",
    "                   for token in tokens]\n",
    "    \n",
    "    # Convert to tensor and add batch dimension\n",
    "    src = torch.LongTensor(src_indices).unsqueeze(0).to(device)\n",
    "    src_mask = (src != en_vocab.stoi[PAD_TOKEN]).unsqueeze(1)\n",
    "    \n",
    "    # Autoregressive generation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Initialize decoder with SOS token\n",
    "        decoder_input = torch.LongTensor([[de_vocab.stoi[SOS_TOKEN]]]).to(device)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Create target mask (padding mask only)\n",
    "            tgt_mask = (decoder_input != de_vocab.stoi[PAD_TOKEN]).unsqueeze(1)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src, decoder_input, src_mask, tgt_mask, src_mask)\n",
    "            \n",
    "            # Get most likely next token\n",
    "            next_token = output.argmax(dim=-1)[:, -1].unsqueeze(1)\n",
    "            \n",
    "            # Stop if EOS is generated\n",
    "            if next_token.item() == de_vocab.stoi[EOS_TOKEN]:\n",
    "                break\n",
    "                \n",
    "            decoder_input = torch.cat([decoder_input, next_token], dim=-1)\n",
    "    \n",
    "    # Convert indices to tokens\n",
    "    translated_indices = decoder_input[0, 1:].cpu().tolist()  # Remove SOS\n",
    "    translated_tokens = []\n",
    "    for idx in translated_indices:\n",
    "        if idx == de_vocab.stoi[EOS_TOKEN]:\n",
    "            break\n",
    "        translated_tokens.append(de_vocab.itos[idx])\n",
    "    \n",
    "    return ' '.join(translated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_sentence(model, \"I love you\", en_vocab, de_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
